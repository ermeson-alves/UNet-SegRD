{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b3034aa",
   "metadata": {},
   "source": [
    "# Estrutura:\n",
    "\n",
    "* [Tratamento de Dados](#dados)\n",
    "* [Integração com o Antigo Notebook](#integracao)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "521b4836-a40f-4919-b0b5-0c648aa46510",
   "metadata": {},
   "source": [
    "# Data <a id=\"dados\"><a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0c82bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/06/image-recognition-using-pytorch-lightning/\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "import torchvision\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "from random import randint\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import monai\n",
    "import pandas as pd\n",
    "import torchio as tio\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from config import *\n",
    "import albumentations as A\n",
    "monai.config.print_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ea6147e",
   "metadata": {},
   "source": [
    "# Load dataset <a id = \"torchdataset\"><a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0b51999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data train \n",
    "path_dir_train = 'datasets/diaretdb1_v_1_1/TRAINSET/ddb1_fundusimages'\n",
    "path_mask_train = 'datasets/diaretdb1_v_1_1/TRAINSET/ddb1_groundtruth/hardexudates'\n",
    "img_list_train = os.listdir(os.path.join(path_dir_train))\n",
    "\n",
    "# Data test\n",
    "path_dir_test = 'datasets/diaretdb1_v_1_1/TESTSET/ddb1_fundusimages'\n",
    "path_mask_test = 'datasets/diaretdb1_v_1_1/TESTSET/ddb1_groundtruth/hardexudates'\n",
    "img_list_test = os.listdir(os.path.join(path_dir_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a0a82815",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = {\n",
    "    'test': transforms.Compose([\n",
    "                        transforms.ToPILImage(),\n",
    "                        transforms.Resize((224,224)),\n",
    "                        # transforms.Grayscale(1),\n",
    "                        transforms.ToTensor()]),\n",
    "     'train': transforms.Compose([\n",
    "                           transforms.ToPILImage(),\n",
    "                           transforms.Resize((224,224)),\n",
    "                           transforms.RandomRotation(degrees = 45),\n",
    "                           transforms.RandomHorizontalFlip(p = 0.005),\n",
    "                        #    transforms.Grayscale(1),\n",
    "                           transforms.ToTensor(),\n",
    "                           A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                           ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "da116de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RD_SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_list, image_dir, mask_dir, transform=None, target_transform=None):\n",
    "        \n",
    "        self.data_list = img_list\n",
    "        self.data_img = image_dir\n",
    "        self.data_mask = mask_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        path_image = os.path.join(self.data_img, self.data_list[idx])\n",
    "        image = cv2.imread(path_image)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    \n",
    "        h, w, c = np.shape(image)\n",
    "        image= image.reshape(c,h,w)\n",
    "        print(image.shape)\n",
    "        path_mask = os.path.join(self.data_mask, self.data_list[idx])\n",
    "        mask = cv2.imread(path_mask,0) \n",
    "        # h1, w1 = np.shape(mask)\n",
    "        # mask = np.reshape(h1,w1)\n",
    "        mask = cv2.threshold(mask, 240, 255, cv2.THRESH_BINARY)[1]\n",
    "        mask = mask/255\n",
    "        mask = np.expand_dims(mask, 2)\n",
    "\n",
    "\n",
    "        \n",
    "        # if self.transform:   \n",
    "        #     # transformed = self.transform(image=image, mask=mask) \n",
    "        #     transformed = self.transform(image=image, mask=mask) \n",
    "        #     image = transformed[\"image\"] \n",
    "        #     mask = transformed[\"mask\"] \n",
    "        #     mask = mask.permute(2,0,1)\n",
    "\n",
    "\n",
    "        return {'image':image, 'mask':mask}\n",
    "    \n",
    "RD_dataset = RD_SegmentationDataset(img_list_train, path_dir_train, path_mask_train, transformations['train'])\n",
    "RD_dataloader = DataLoader(RD_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "627bcbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "torch.uint8\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of training data\n",
    "batch = next(iter(RD_dataloader))\n",
    "inputs = batch['image']\n",
    "print(inputs.dtype)\n",
    "masks = batch['mask']\n",
    "\n",
    "# fig, axs  = plt.subplots(4,2, figsize=(8,10))\n",
    "# for ax, i in zip(axs, range(len(axs))):\n",
    "#     ax[0].imshow(inputs[i])\n",
    "#     ax[1].imshow(masks[i], cmap=\"gray\")\n",
    "# fig.suptitle(\"Lote de amostra de treino\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3587b8c4",
   "metadata": {},
   "source": [
    "# Setup Enviroment <a id=\"integracao\"><a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "65f0aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly\"\n",
    "!pip install -q torch==1.10.2 torchtext==0.11.2 torchvision==0.11.3\n",
    "!pip install -q torchio==0.18.73\n",
    "!pip install -q pytorch-lightning==1.5.10\n",
    "!pip install -q pandas==1.1.5 seaborn==0.11.1\n",
    "!pip install -q pillow==9.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11e982",
   "metadata": {},
   "source": [
    "# Configurations and Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "38017045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "sns.set()\n",
    "plt.rcParams[\"figure.figsize\"] = 12, 8\n",
    "monai.utils.set_determinism()\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdb79b1e",
   "metadata": {},
   "source": [
    "# Setup Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "04b9f114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/diaretdb1_v_1_1\n"
     ]
    }
   ],
   "source": [
    "# directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "# dataset_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "# print(dataset_dir)\n",
    "\n",
    "dataset_dir = \"./datasets/diaretdb1_v_1_1\"\n",
    "print(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b2216",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffcd334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MedicalDecathlonDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, dataset_name, batch_size, train_val_ratio, lesion):\n",
    "#         '''lesion: EX: hardexsudates, HE: hemorrhages, MA: microaneurysms, SE'''\n",
    "#         super().__init__()\n",
    "#         self.dataset_name = dataset_name\n",
    "#         self.batch_size = batch_size\n",
    "#         self.base_dir = root_dir\n",
    "#         self.dataset_dir = os.path.join(root_dir, dataset_name)\n",
    "#         self.lesion = lesion\n",
    "#         self.train_val_ratio = train_val_ratio\n",
    "#         self.subjects = None\n",
    "#         self.test_subjects = None\n",
    "#         self.preprocess = None\n",
    "#         self.transform = None\n",
    "#         self.train_set = None\n",
    "#         self.val_set = None\n",
    "#         self.test_set = None\n",
    "\n",
    "    # def download_data(self):\n",
    "    #     if not os.path.isdir(self.dataset_dir):\n",
    "    #         url = \"https://www.it.lut.fi/project/imageret/diaretdb1/diaretdb1_v_1_1.zip\"\n",
    "    #         monai.apps.download_and_extract(url, output_dir=\"./datasets\")\n",
    "    #         # TESTSET:\n",
    "    #         adaptar_dataset(ROOT_DATASET_PATH, IMGS_FUNDUS_PATH, MASKS_DIR_PATH, ANNOTATIONS_TEST_PATH)\n",
    "    #         # TRAINSET:\n",
    "    #         adaptar_dataset(ROOT_DATASET_PATH, IMGS_FUNDUS_PATH, MASKS_DIR_PATH, ANNOTATIONS_TRAIN_PATH)\n",
    "\n",
    "    #     mask_dir = sorted(os.listdir(os.path.join(self.dataset_dir, \"TRAINSET\", \"ddb1_groundtruth\")))[LESIONS[self.lesion]]\n",
    "\n",
    "    #     image_training_paths = sorted(glob(os.path.join(self.dataset_dir, \"TRAINSET\", \"ddb1_fundusimages\", \"*.png\")))\n",
    "    #     label_training_paths = sorted(glob(os.path.join(self.dataset_dir, \"TRAINSET\", \"ddb1_groundtruth\", mask_dir, \"*.png\")))\n",
    "    #     image_test_paths = sorted(glob(os.path.join(self.dataset_dir, \"TESTSET\", \"ddb1_fundusimages\", \"*.png*\")))\n",
    "    #     print(image_training_paths, label_training_paths, image_test_paths)\n",
    "    #     return image_training_paths, label_training_paths, image_test_paths\n",
    "    \n",
    "    \n",
    "    # def prepare_data(self):\n",
    "    #     '''como baixar, tokenizar, etc…'''\n",
    "    #     image_training_paths, label_training_paths, image_test_paths = self.download_data()\n",
    "\n",
    "    #     self.subjects = []\n",
    "    #     for image_path, label_path in zip(image_training_paths, label_training_paths):\n",
    "    #         # 'image' and 'label' are arbitrary names for the images\n",
    "    #         subject = tio.Subject(image=tio.ScalarImage(image_path), label=tio.LabelMap(label_path))\n",
    "    #         self.subjects.append(subject)\n",
    "\n",
    "    #     self.test_subjects = []\n",
    "    #     for image_path in image_test_paths:\n",
    "    #         subject = tio.Subject(image=tio.ScalarImage(image_path))\n",
    "    #         self.test_subjects.append(subject)\n",
    "\n",
    "    # def get_preprocessing_transform(self):\n",
    "    #     preprocess = tio.Compose(\n",
    "    #         [\n",
    "    #             tio.RescaleIntensity((0, 1)),\n",
    "    #             # tio.CropOrPad(self.get_max_shape(self.subjects + self.test_subjects)),\n",
    "    #             tio.EnsureShapeMultiple(8),  # for the U-Net\n",
    "    #             tio.OneHot(),\n",
    "    #         ]\n",
    "    #     )\n",
    "    #     return preprocess\n",
    "\n",
    "    # def get_augmentation_transform(self):\n",
    "    #     augment = tio.Compose(\n",
    "    #         [\n",
    "    #             tio.RandomAffine(),\n",
    "    #             # tio.RandomGamma(p=0.5),\n",
    "    #             # tio.RandomNoise(p=0.5),\n",
    "    #             # tio.RandomMotion(p=0.1),\n",
    "    #             # tio.RandomBiasField(p=0.25),\n",
    "    #         ]\n",
    "    #     )\n",
    "    #     return augment\n",
    " \n",
    "\n",
    "    # def setup(self, stage=None):\n",
    "    #     '''como dividir, definir conjunto de dados, etc…'''\n",
    "    #     # self.transforms =   data_transforms = { \n",
    "    #     #                     'val': A.Compose([ \n",
    "    #     #                         A.Resize(224, 224),  \n",
    "    #     #                         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  \n",
    "    #     #                         # ToTensorV2() \n",
    "    #     #                     ]), \n",
    "    #     #                     }\n",
    "    #     num_subjects = len(self.subjects)\n",
    "    #     num_train_subjects = int(round(num_subjects * self.train_val_ratio))\n",
    "    #     num_val_subjects = num_subjects - num_train_subjects\n",
    "    #     splits = num_train_subjects, num_val_subjects\n",
    "    #     train_subjects, val_subjects = random_split(self.subjects, splits)\n",
    "\n",
    "    #     self.preprocess = self.get_preprocessing_transform()\n",
    "    #     augment = self.get_augmentation_transform()\n",
    "    #     self.transform = tio.Compose([self.preprocess, augment])\n",
    "\n",
    "    #     self.train_set = tio.SubjectsDataset(train_subjects, transform=self.transform)\n",
    "    #     self.val_set = tio.SubjectsDataset(val_subjects, transform=self.preprocess)\n",
    "    #     self.test_set = tio.SubjectsDataset(self.test_subjects, transform=self.preprocess)\n",
    "\n",
    "    # def train_dataloader(self):\n",
    "    #     return DataLoader(self.train_set, self.batch_size, num_workers=2)\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return DataLoader(self.val_set, self.batch_size, num_workers=2)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.test_set, self.batch_size, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b33feeb",
   "metadata": {},
   "source": [
    "[Definição da Classe do Dataset](#torchdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c10d3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = RD_SegmentationDataset(img_list = img_list_train, \n",
    "                                   image_dir = path_dir_train, \n",
    "                                   mask_dir = path_mask_train, \n",
    "                                   transform = transformations['train'])\n",
    "test_set = RD_SegmentationDataset(img_list = img_list_test, \n",
    "                                   image_dir = path_dir_test, \n",
    "                                   mask_dir = path_mask_test, \n",
    "                                   transform = transformations['test'])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0ad0a",
   "metadata": {},
   "source": [
    "# Lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5551864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, net, criterion, learning_rate, optimizer_class):\n",
    "        super().__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.net = net\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_class = optimizer_class\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_class(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def prepare_batch(self, batch):\n",
    "\n",
    "        return batch[\"image\"] , batch[\"mask\"]\n",
    "\n",
    "    def infer_batch(self, batch):\n",
    "        x, y = self.prepare_batch(batch)\n",
    "        print(x.dtype, y.dtype)\n",
    "        y_hat = self.net(x)\n",
    "        return y_hat, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat, y = self.infer_batch(batch)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat, y = self.infer_batch(batch)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "01aab168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "unet = monai.networks.nets.UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    channels=(8, 16, 32, 64),\n",
    "    strides=(2, 2, 2),\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    net=unet,\n",
    "    criterion=monai.losses.DiceCELoss(softmax=True),\n",
    "    learning_rate=1e-2,\n",
    "    optimizer_class=torch.optim.AdamW,\n",
    ")\n",
    "early_stopping = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,\n",
    "    # precision='bf16',\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "trainer.logger._default_hp_metric = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c5a592",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "62572112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:122: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | net       | UNet       | 41.0 K\n",
      "1 | criterion | DiceCELoss | 0     \n",
      "-----------------------------------------\n",
      "41.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "41.0 K    Total params\n",
      "0.164     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at 2023-04-05 16:11:50.982345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:432: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c8af74f7a247bd86eeb912a10dbeba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "(3, 576, 750)\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Byte but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining started at\u001b[39m\u001b[39m\"\u001b[39m, start)\n\u001b[1;32m----> 3\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mmodel, train_dataloaders\u001b[39m=\u001b[39;49mtrain_loader)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining duration:\u001b[39m\u001b[39m\"\u001b[39m, datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[0;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[0;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m     )\n\u001b[0;32m    739\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[1;32m--> 740\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    741\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    742\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 685\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    686\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[0;32m    776\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m--> 777\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    779\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m   1198\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[1;32m-> 1199\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[0;32m   1201\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1279\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1318\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m data_fetcher \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[0;32m    233\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(data_fetcher)\n\u001b[0;32m    236\u001b[0m     \u001b[39m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[39m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[39m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:193\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m    192\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 193\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(batch, batch_idx)\n\u001b[0;32m    195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    197\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m     87\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[1;32m---> 88\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(split_batch, optimizers, batch_idx)\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(split_batch, batch_idx)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:215\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[1;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(\n\u001b[0;32m    216\u001b[0m         batch,\n\u001b[0;32m    217\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_idx,\n\u001b[0;32m    218\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position],\n\u001b[0;32m    219\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_idx,\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[0;32m    223\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[0;32m    224\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:266\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[1;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[0;32m    259\u001b[0m         closure()\n\u001b[0;32m    261\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, batch_idx, closure)\n\u001b[0;32m    268\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:378\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    377\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m lightning_module\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[0;32m    379\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    380\u001b[0m     batch_idx,\n\u001b[0;32m    381\u001b[0m     optimizer,\n\u001b[0;32m    382\u001b[0m     opt_idx,\n\u001b[0;32m    383\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    384\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_device_type \u001b[39m==\u001b[39;49m DeviceType\u001b[39m.\u001b[39;49mTPU \u001b[39mand\u001b[39;49;00m _TPU_AVAILABLE),\n\u001b[0;32m    385\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mand\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[0;32m    386\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[0;32m    387\u001b[0m )\n\u001b[0;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:1652\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1572\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1573\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1580\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1581\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1582\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m \u001b[39m    Override this method to adjust the default way the\u001b[39;00m\n\u001b[0;32m   1584\u001b[0m \u001b[39m    :class:`~pytorch_lightning.trainer.trainer.Trainer` calls each optimizer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1650\u001b[0m \n\u001b[0;32m   1651\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1652\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:164\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39massert\u001b[39;00m trainer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(profiler_action):\n\u001b[1;32m--> 164\u001b[0m     trainer\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39moptimizer_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_idx, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py:339\u001b[0m, in \u001b[0;36mAccelerator.optimizer_step\u001b[1;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"performs the actual optimizer step.\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \n\u001b[0;32m    331\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[1;32m--> 339\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39moptimizer_step(model, optimizer, opt_idx, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:163\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m    162\u001b[0m     closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[1;32m--> 163\u001b[0m optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\optim\\adamw.py:92\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m---> 92\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m     95\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:148\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m    136\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    137\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    141\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    142\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[0;32m    145\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[0;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:160\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclosure(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:142\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m    141\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mtraining_step_and_backward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m         step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[0;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    146\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:435\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[1;34m(self, split_batch, batch_idx, opt_idx)\u001b[0m\n\u001b[0;32m    433\u001b[0m lightning_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 435\u001b[0m     training_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mtraining_step(step_kwargs)\n\u001b[0;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mpost_training_step()\n\u001b[0;32m    438\u001b[0m \u001b[39mdel\u001b[39;00m step_kwargs\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py:219\u001b[0m, in \u001b[0;36mAccelerator.training_step\u001b[1;34m(self, step_kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The actual training step.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[39mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.training_step` for more details\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[1;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py:213\u001b[0m, in \u001b[0;36mTrainingTypePlugin.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[115], line 27\u001b[0m, in \u001b[0;36mModel.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m---> 27\u001b[0m     y_hat, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer_batch(batch)\n\u001b[0;32m     28\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(y_hat, y)\n\u001b[0;32m     29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m, loss, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[115], line 22\u001b[0m, in \u001b[0;36mModel.infer_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     20\u001b[0m x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_batch(batch)\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(x), \u001b[39mtype\u001b[39m(y))\n\u001b[1;32m---> 22\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n\u001b[0;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m y_hat, y\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\monai\\networks\\nets\\unet.py:303\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 303\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[0;32m    304\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\LESC\\Desktop\\UNet-SegRD\\lesc-env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    440\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    441\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    443\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Byte but found Float"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"Training started at\", start)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n",
    "\n",
    "print(\"Training duration:\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8270fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c1af5",
   "metadata": {},
   "source": [
    "# Plot validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "all_dices = []\n",
    "get_dice = monai.metrics.DiceMetric(include_background=False, reduction=\"none\")\n",
    "with torch.no_grad():\n",
    "    for batch in data.val_dataloader():\n",
    "        inputs, targets = model.prepare_batch(batch)\n",
    "        logits = model.net(inputs.to(model.device))\n",
    "        labels = logits.argmax(dim=1)\n",
    "        labels_one_hot = torch.nn.functional.one_hot(labels).permute(0, 4, 1, 2, 3)\n",
    "        get_dice(labels_one_hot.to(model.device), targets.to(model.device))\n",
    "    metric = get_dice.aggregate()\n",
    "    get_dice.reset()\n",
    "    all_dices.append(metric)\n",
    "all_dices = torch.cat(all_dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for ant, post in all_dices:\n",
    "    records.append({\"Dice\": ant, \"Label\": \"Anterior\"})\n",
    "    records.append({\"Dice\": post, \"Label\": \"Posterior\"})\n",
    "df = pd.DataFrame.from_records(records)\n",
    "ax = sns.stripplot(x=\"Label\", y=\"Dice\", data=df, size=10, alpha=0.5)\n",
    "ax.set_title(\"Dice scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ccc548",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in data.test_dataloader():\n",
    "        inputs = batch[\"image\"][tio.DATA].to(model.device)\n",
    "        labels = model.net(inputs).argmax(dim=1, keepdim=True).cpu()\n",
    "        break\n",
    "batch_subjects = tio.utils.get_subjects_from_batch(batch)\n",
    "tio.utils.add_images_from_batch(batch_subjects, labels, tio.LabelMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in batch_subjects:\n",
    "    subject.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lesc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7561ee3bf7283a7ee20208c9de9e891ea891fd52bb99e81fee2c25f671b62ad4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
